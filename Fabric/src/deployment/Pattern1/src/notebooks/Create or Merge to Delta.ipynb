{"nbformat":4,"nbformat_minor":5,"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","display_name":"Synapse PySpark"},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"},"host":{}},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"trident":{"lakehouse":{"known_lakehouses":[{"id":"c02dea28-20ca-432b-b6e8-39d0be76f540"}]}}},"cells":[{"cell_type":"code","source":["# Welcome to your new notebook\n","# Type here in the cell editor to add code!\n","from delta.tables import *\n","from pyspark.sql.functions import *\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"163b879d-ad53-4620-a4d6-068e682532b0","statement_id":3,"state":"finished","livy_statement_state":"available","queued_time":"2023-06-08T14:42:18.6186424Z","session_start_time":"2023-06-08T14:42:18.7927822Z","execution_start_time":"2023-06-08T14:42:27.6417091Z","execution_finish_time":"2023-06-08T14:42:29.3558843Z","spark_jobs":{"numbers":{"FAILED":0,"SUCCEEDED":0,"UNKNOWN":0,"RUNNING":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"75728798-8f3f-4d20-8e0b-65d061473677"},"text/plain":"StatementMeta(, 163b879d-ad53-4620-a4d6-068e682532b0, 3, Finished, Available)"},"metadata":{}}],"execution_count":1,"metadata":{},"id":"ccd22b05-d123-4760-b29a-deca32ea4484"},{"cell_type":"code","source":["lakehousePath = \"abfss://85bfc254-9abf-46cc-b1fe-943ec35b3460@msit-onelake.dfs.fabric.microsoft.com/c02dea28-20ca-432b-b6e8-39d0be76f540\"\r\n","tableName = \"Invoices\"\r\n","tableKey = \"InvoiceID\"\r\n","dateColumn = \"LastEditedWhen\""],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"163b879d-ad53-4620-a4d6-068e682532b0","statement_id":4,"state":"finished","livy_statement_state":"available","queued_time":"2023-06-08T14:42:37.2509779Z","session_start_time":null,"execution_start_time":"2023-06-08T14:42:37.5237582Z","execution_finish_time":"2023-06-08T14:42:37.8593865Z","spark_jobs":{"numbers":{"FAILED":0,"SUCCEEDED":0,"UNKNOWN":0,"RUNNING":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"dfb6f1e6-6c03-4953-96bb-3e99af6634a6"},"text/plain":"StatementMeta(, 163b879d-ad53-4620-a4d6-068e682532b0, 4, Finished, Available)"},"metadata":{}}],"execution_count":2,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"tags":["parameters"]},"id":"d07e2fd8-7de3-41ad-aece-efa01fefb2bf"},{"cell_type":"code","source":["deltaTablePath = f\"{lakehousePath}/Tables/{tableName}\" \r\n","# print(deltaTablePath)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"163b879d-ad53-4620-a4d6-068e682532b0","statement_id":5,"state":"finished","livy_statement_state":"available","queued_time":"2023-06-08T14:43:00.0878307Z","session_start_time":null,"execution_start_time":"2023-06-08T14:43:00.391446Z","execution_finish_time":"2023-06-08T14:43:00.7304706Z","spark_jobs":{"numbers":{"FAILED":0,"SUCCEEDED":0,"UNKNOWN":0,"RUNNING":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"68ec9d9c-977d-49f4-b9eb-6d62df5376c2"},"text/plain":"StatementMeta(, 163b879d-ad53-4620-a4d6-068e682532b0, 5, Finished, Available)"},"metadata":{}}],"execution_count":3,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"b59e5ade-dd4b-49bf-a9d3-0e476a223f44"},{"cell_type":"code","source":["parquetFilePath = f\"{lakehousePath}/Files/incremental/{tableName}/{tableName}.parquet\"\r\n","# print(parquetFilePath)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"e3bfaad7-54ed-4f11-a2bb-307e2a9d49f3","statement_id":11,"state":"finished","livy_statement_state":"available","queued_time":"2023-06-06T17:40:37.5559293Z","session_start_time":null,"execution_start_time":"2023-06-06T17:40:39.3671281Z","execution_finish_time":"2023-06-06T17:40:39.6722717Z","spark_jobs":{"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":0,"UNKNOWN":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"e6dadc26-01e4-4200-844f-e3a47ba8ef45"},"text/plain":"StatementMeta(, e3bfaad7-54ed-4f11-a2bb-307e2a9d49f3, 11, Finished, Available)"},"metadata":{}}],"execution_count":9,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"5edf1ff1-9592-4d2e-be58-8880457e8f7c"},{"cell_type":"code","source":["df2 = spark.read.parquet(parquetFilePath)\n","# display(df2)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"e3bfaad7-54ed-4f11-a2bb-307e2a9d49f3","statement_id":12,"state":"finished","livy_statement_state":"available","queued_time":"2023-06-06T17:40:37.7555102Z","session_start_time":null,"execution_start_time":"2023-06-06T17:40:39.9934476Z","execution_finish_time":"2023-06-06T17:40:41.7205576Z","spark_jobs":{"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":1,"UNKNOWN":0},"jobs":[{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":7,"name":"parquet at NativeMethodAccessorImpl.java:0","description":"Job group for statement 12:\ndf2 = spark.read.parquet(parquetFilePath)\n# display(df2)","submissionTime":"2023-06-06T17:40:40.354GMT","completionTime":"2023-06-06T17:40:41.045GMT","stageIds":[10],"jobGroup":"12","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"7f6515d0-553b-459e-97e4-051a50aff1a0"},"text/plain":"StatementMeta(, e3bfaad7-54ed-4f11-a2bb-307e2a9d49f3, 12, Finished, Available)"},"metadata":{}}],"execution_count":10,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"f09d758c-b26b-4d3f-a440-03ea19683d1d"},{"cell_type":"markdown","source":["Check if table already exists; if it does, do an upsert and return how many rows were inserted and update; if it does not exist, return how many rows were inserted"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"56fa0583-585e-429f-b09c-71b16843045f"},{"cell_type":"code","source":["if DeltaTable.isDeltaTable(spark,deltaTablePath):\r\n","    deltaTable = DeltaTable.forPath(spark,deltaTablePath)\r\n","    deltaTable.alias(\"t\").merge(\r\n","        df2.alias(\"s\"),\r\n","        f\"t.{tableKey} = s.{tableKey}\"\r\n","    ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\r\n","    history = deltaTable.history(1).select(\"operationMetrics\")\r\n","    operationMetrics = history.collect()[0][\"operationMetrics\"]\r\n","    numInserted = operationMetrics[\"numTargetRowsInserted\"]\r\n","    numUpdated = operationMetrics[\"numTargetRowsUpdated\"]\r\n","else:\r\n","    df2.write.format(\"delta\").save(deltaTablePath)  \r\n","    deltaTable = DeltaTable.forPath(spark,deltaTablePath)\r\n","    operationMetrics = history.collect()[0][\"operationMetrics\"]\r\n","    numInserted = operationMetrics[\"numTargetRowsInserted\"]\r\n","    numUpdated = 0"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"e3bfaad7-54ed-4f11-a2bb-307e2a9d49f3","statement_id":13,"state":"finished","livy_statement_state":"available","queued_time":"2023-06-06T17:40:37.9014052Z","session_start_time":null,"execution_start_time":"2023-06-06T17:40:42.0430042Z","execution_finish_time":"2023-06-06T17:40:52.6378532Z","spark_jobs":{"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":14,"UNKNOWN":0},"jobs":[{"dataWritten":0,"dataRead":0,"rowCount":1,"jobId":22,"name":"getHistory at DeltaTableOperations.scala:54","description":"Job group for statement 13:\nif DeltaTable.isDeltaTable(spark,deltaTablePath):\n    deltaTable = DeltaTable.forPath(spark,deltaTablePath)\n    deltaTable.alias(\"t\").merge(\n        df2.alias(\"s\"),\n        f\"t.{tableKey} = s.{tableKey}\"\n    ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n    history = deltaTable.history(1).select(\"operationMetrics\")\n    operationMetrics = history.collect()[0][\"operationMetrics\"]\n    numInserted = operationMetrics[\"numTargetRowsInserted\"]\n    numUpdated = operationMetrics[\"numTargetRowsUpdated\"]\nelse:\n    df2.write.format(\"delta\").save(deltaTablePath)  \n    deltaTable = DeltaTable.forPath(spark,deltaTablePath)\n    operationMetrics = history.collect()[0][\"operationMetrics\"]\n    numInserted = operationMetrics[\"numTargetRowsInserted\"]\n    numUpdated = 0","submissionTime":"2023-06-06T17:40:51.411GMT","completionTime":"2023-06-06T17:40:51.699GMT","stageIds":[35],"jobGroup":"13","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":8,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":8,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":4924,"rowCount":50,"jobId":21,"name":"toString at String.java:2994","description":"Delta: Job group for statement 13:\nif DeltaTable.isDeltaTable(spark,deltaTablePath):\n    deltaTable = DeltaTable.forPath(spark,deltaTablePath)\n    deltaTable.alias(\"t\").merge(\n        df2.alias(\"s\"),\n        f\"t.{tableKey} = s.{tableKey}\"\n    ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n    history = deltaTable.history(1).select(\"operationMetrics\")\n    operationMetrics = history.collect()[0][\"operationMetrics\"]\n    numInserted = operationMetrics[\"numTargetRowsInserted\"]\n    numUpdated = operationMetrics[\"numTargetRowsUpdated\"]\nelse:\n    df2.write.format(\"delta\").save(deltaTablePath)  \n    deltaTable = DeltaTable.forPath(spark,deltaTablePath)\n    operationMetrics = history.collect()[0][\"operationMetrics\"]\n    numInserted = operationMetrics[\"numTargetRowsInserted\"]\n    numUpdated = 0: Compute snapshot for version: 23","submissionTime":"2023-06-06T17:40:51.063GMT","completionTime":"2023-06-06T17:40:51.110GMT","stageIds":[33,34,32],"jobGroup":"13","status":"SUCCEEDED","numTasks":55,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":54,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4924,"dataRead":29831,"rowCount":100,"jobId":20,"name":"toString at String.java:2994","description":"Delta: Job group for statement 13:\nif DeltaTable.isDeltaTable(spark,deltaTablePath):\n    deltaTable = DeltaTable.forPath(spark,deltaTablePath)\n    deltaTable.alias(\"t\").merge(\n        df2.alias(\"s\"),\n        f\"t.{tableKey} = s.{tableKey}\"\n    ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n    history = deltaTable.history(1).select(\"operationMetrics\")\n    operationMetrics = history.collect()[0][\"operationMetrics\"]\n    numInserted = operationMetrics[\"numTargetRowsInserted\"]\n    numUpdated = operationMetrics[\"numTargetRowsUpdated\"]\nelse:\n    df2.write.format(\"delta\").save(deltaTablePath)  \n    deltaTable = DeltaTable.forPath(spark,deltaTablePath)\n    operationMetrics = history.collect()[0][\"operationMetrics\"]\n    numInserted = operationMetrics[\"numTargetRowsInserted\"]\n    numUpdated = 0: Compute snapshot for version: 23","submissionTime":"2023-06-06T17:40:50.523GMT","completionTime":"2023-06-06T17:40:51.040GMT","stageIds":[30,31],"jobGroup":"13","status":"SUCCEEDED","numTasks":54,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":4,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":29831,"dataRead":41759,"rowCount":100,"jobId":19,"name":"toString at String.java:2994","description":"Delta: Job group for statement 13:\nif DeltaTable.isDeltaTable(spark,deltaTablePath):\n    deltaTable = DeltaTable.forPath(spark,deltaTablePath)\n    deltaTable.alias(\"t\").merge(\n        df2.alias(\"s\"),\n        f\"t.{tableKey} = s.{tableKey}\"\n    ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n    history = deltaTable.history(1).select(\"operationMetrics\")\n    operationMetrics = history.collect()[0][\"operationMetrics\"]\n    numInserted = operationMetrics[\"numTargetRowsInserted\"]\n    numUpdated = operationMetrics[\"numTargetRowsUpdated\"]\nelse:\n    df2.write.format(\"delta\").save(deltaTablePath)  \n    deltaTable = DeltaTable.forPath(spark,deltaTablePath)\n    operationMetrics = history.collect()[0][\"operationMetrics\"]\n    numInserted = operationMetrics[\"numTargetRowsInserted\"]\n    numUpdated = 0: Compute snapshot for version: 23","submissionTime":"2023-06-06T17:40:50.230GMT","completionTime":"2023-06-06T17:40:50.358GMT","stageIds":[29],"jobGroup":"13","status":"SUCCEEDED","numTasks":4,"numActiveTasks":0,"numCompletedTasks":4,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":4,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":28342,"dataRead":84682,"rowCount":309,"jobId":18,"name":"run at ForkJoinTask.java:1402","description":"Delta: Job group for statement 13:\nif DeltaTable.isDeltaTable(spark,deltaTablePath):\n    deltaTable = DeltaTable.forPath(spark,deltaTablePath)\n    deltaTable.alias(\"t\").merge(\n        df2.alias(\"s\"),\n        f\"t.{tableKey} = s.{tableKey}\"\n    ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n    history = deltaTable.history(1).select(\"operationMetrics\")\n    operationMetrics = history.collect()[0][\"operationMetrics\"]\n    numInserted = operationMetrics[\"numTargetRowsInserted\"]\n    numUpdated = operationMetrics[\"numTargetRowsUpdated\"]\nelse:\n    df2.write.format(\"delta\").save(deltaTablePath)  \n    deltaTable = DeltaTable.forPath(spark,deltaTablePath)\n    operationMetrics = history.collect()[0][\"operationMetrics\"]\n    numInserted = operationMetrics[\"numTargetRowsInserted\"]\n    numUpdated = 0: Writing merged data full update","submissionTime":"2023-06-06T17:40:48.791GMT","completionTime":"2023-06-06T17:40:49.578GMT","stageIds":[27,28,26],"jobGroup":"13","status":"SUCCEEDED","numTasks":3,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":2,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":42341,"dataRead":24592,"rowCount":206,"jobId":17,"name":"run at ForkJoinTask.java:1402","description":"Delta: Job group for statement 13:\nif DeltaTable.isDeltaTable(spark,deltaTablePath):\n    deltaTable = DeltaTable.forPath(spark,deltaTablePath)\n    deltaTable.alias(\"t\").merge(\n        df2.alias(\"s\"),\n        f\"t.{tableKey} = s.{tableKey}\"\n    ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n    history = deltaTable.history(1).select(\"operationMetrics\")\n    operationMetrics = history.collect()[0][\"operationMetrics\"]\n    numInserted = operationMetrics[\"numTargetRowsInserted\"]\n    numUpdated = operationMetrics[\"numTargetRowsUpdated\"]\nelse:\n    df2.write.format(\"delta\").save(deltaTablePath)  \n    deltaTable = DeltaTable.forPath(spark,deltaTablePath)\n    operationMetrics = history.collect()[0][\"operationMetrics\"]\n    numInserted = operationMetrics[\"numTargetRowsInserted\"]\n    numUpdated = 0: Writing merged data full update","submissionTime":"2023-06-06T17:40:48.043GMT","completionTime":"2023-06-06T17:40:48.364GMT","stageIds":[25],"jobGroup":"13","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":2884,"dataRead":27472,"rowCount":103,"jobId":16,"name":"run at ForkJoinTask.java:1402","description":"Delta: Job group for statement 13:\nif DeltaTable.isDeltaTable(spark,deltaTablePath):\n    deltaTable = DeltaTable.forPath(spark,deltaTablePath)\n    deltaTable.alias(\"t\").merge(\n        df2.alias(\"s\"),\n        f\"t.{tableKey} = s.{tableKey}\"\n    ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n    history = deltaTable.history(1).select(\"operationMetrics\")\n    operationMetrics = history.collect()[0][\"operationMetrics\"]\n    numInserted = operationMetrics[\"numTargetRowsInserted\"]\n    numUpdated = operationMetrics[\"numTargetRowsUpdated\"]\nelse:\n    df2.write.format(\"delta\").save(deltaTablePath)  \n    deltaTable = DeltaTable.forPath(spark,deltaTablePath)\n    operationMetrics = history.collect()[0][\"operationMetrics\"]\n    numInserted = operationMetrics[\"numTargetRowsInserted\"]\n    numUpdated = 0: Writing merged no shuffle data","submissionTime":"2023-06-06T17:40:47.901GMT","completionTime":"2023-06-06T17:40:48.745GMT","stageIds":[24],"jobGroup":"13","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":42341,"dataRead":16089,"rowCount":206,"jobId":15,"name":"run at ForkJoinTask.java:1402","description":"Delta: Job group for statement 13:\nif DeltaTable.isDeltaTable(spark,deltaTablePath):\n    deltaTable = DeltaTable.forPath(spark,deltaTablePath)\n    deltaTable.alias(\"t\").merge(\n        df2.alias(\"s\"),\n        f\"t.{tableKey} = s.{tableKey}\"\n    ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n    history = deltaTable.history(1).select(\"operationMetrics\")\n    operationMetrics = history.collect()[0][\"operationMetrics\"]\n    numInserted = operationMetrics[\"numTargetRowsInserted\"]\n    numUpdated = operationMetrics[\"numTargetRowsUpdated\"]\nelse:\n    df2.write.format(\"delta\").save(deltaTablePath)  \n    deltaTable = DeltaTable.forPath(spark,deltaTablePath)\n    operationMetrics = history.collect()[0][\"operationMetrics\"]\n    numInserted = operationMetrics[\"numTargetRowsInserted\"]\n    numUpdated = 0: Writing merged data full update","submissionTime":"2023-06-06T17:40:47.821GMT","completionTime":"2023-06-06T17:40:48.042GMT","stageIds":[23],"jobGroup":"13","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":687,"rowCount":1,"jobId":14,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 13:\nif DeltaTable.isDeltaTable(spark,deltaTablePath):\n    deltaTable = DeltaTable.forPath(spark,deltaTablePath)\n    deltaTable.alias(\"t\").merge(\n        df2.alias(\"s\"),\n        f\"t.{tableKey} = s.{tableKey}\"\n    ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n    history = deltaTable.history(1).select(\"operationMetrics\")\n    operationMetrics = history.collect()[0][\"operationMetrics\"]\n    numInserted = operationMetrics[\"numTargetRowsInserted\"]\n    numUpdated = operationMetrics[\"numTargetRowsUpdated\"]\nelse:\n    df2.write.format(\"delta\").save(deltaTablePath)  \n    deltaTable = DeltaTable.forPath(spark,deltaTablePath)\n    operationMetrics = history.collect()[0][\"operationMetrics\"]\n    numInserted = operationMetrics[\"numTargetRowsInserted\"]\n    numUpdated = 0: Finding touched files - low shuffle merge","submissionTime":"2023-06-06T17:40:46.319GMT","completionTime":"2023-06-06T17:40:47.584GMT","stageIds":[21,22],"jobGroup":"13","status":"SUCCEEDED","numTasks":206,"numActiveTasks":0,"numCompletedTasks":200,"numSkippedTasks":6,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":200,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":687,"dataRead":513216,"rowCount":61171,"jobId":13,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 13:\nif DeltaTable.isDeltaTable(spark,deltaTablePath):\n    deltaTable = DeltaTable.forPath(spark,deltaTablePath)\n    deltaTable.alias(\"t\").merge(\n        df2.alias(\"s\"),\n        f\"t.{tableKey} = s.{tableKey}\"\n    ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n    history = deltaTable.history(1).select(\"operationMetrics\")\n    operationMetrics = history.collect()[0][\"operationMetrics\"]\n    numInserted = operationMetrics[\"numTargetRowsInserted\"]\n    numUpdated = operationMetrics[\"numTargetRowsUpdated\"]\nelse:\n    df2.write.format(\"delta\").save(deltaTablePath)  \n    deltaTable = DeltaTable.forPath(spark,deltaTablePath)\n    operationMetrics = history.collect()[0][\"operationMetrics\"]\n    numInserted = operationMetrics[\"numTargetRowsInserted\"]\n    numUpdated = 0: Finding touched files - low shuffle merge","submissionTime":"2023-06-06T17:40:45.585GMT","completionTime":"2023-06-06T17:40:46.245GMT","stageIds":[20],"jobGroup":"13","status":"SUCCEEDED","numTasks":6,"numActiveTasks":0,"numCompletedTasks":6,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":6,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":17748,"rowCount":43,"jobId":11,"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 13:\nif DeltaTable.isDeltaTable(spark,deltaTablePath):\n    deltaTable = DeltaTable.forPath(spark,deltaTablePath)\n    deltaTable.alias(\"t\").merge(\n        df2.alias(\"s\"),\n        f\"t.{tableKey} = s.{tableKey}\"\n    ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n    history = deltaTable.history(1).select(\"operationMetrics\")\n    operationMetrics = history.collect()[0][\"operationMetrics\"]\n    numInserted = operationMetrics[\"numTargetRowsInserted\"]\n    numUpdated = operationMetrics[\"numTargetRowsUpdated\"]\nelse:\n    df2.write.format(\"delta\").save(deltaTablePath)  \n    deltaTable = DeltaTable.forPath(spark,deltaTablePath)\n    operationMetrics = history.collect()[0][\"operationMetrics\"]\n    numInserted = operationMetrics[\"numTargetRowsInserted\"]\n    numUpdated = 0: Finding touched files - low shuffle merge","submissionTime":"2023-06-06T17:40:44.569GMT","completionTime":"2023-06-06T17:40:44.826GMT","stageIds":[17,18],"jobGroup":"13","status":"SUCCEEDED","numTasks":53,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":3,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":4925,"rowCount":50,"jobId":10,"name":"toString at String.java:2994","description":"Delta: Job group for statement 13:\nif DeltaTable.isDeltaTable(spark,deltaTablePath):\n    deltaTable = DeltaTable.forPath(spark,deltaTablePath)\n    deltaTable.alias(\"t\").merge(\n        df2.alias(\"s\"),\n        f\"t.{tableKey} = s.{tableKey}\"\n    ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n    history = deltaTable.history(1).select(\"operationMetrics\")\n    operationMetrics = history.collect()[0][\"operationMetrics\"]\n    numInserted = operationMetrics[\"numTargetRowsInserted\"]\n    numUpdated = operationMetrics[\"numTargetRowsUpdated\"]\nelse:\n    df2.write.format(\"delta\").save(deltaTablePath)  \n    deltaTable = DeltaTable.forPath(spark,deltaTablePath)\n    operationMetrics = history.collect()[0][\"operationMetrics\"]\n    numInserted = operationMetrics[\"numTargetRowsInserted\"]\n    numUpdated = 0: Compute snapshot for version: 22","submissionTime":"2023-06-06T17:40:43.955GMT","completionTime":"2023-06-06T17:40:44.016GMT","stageIds":[15,16,14],"jobGroup":"13","status":"SUCCEEDED","numTasks":54,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":53,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4925,"dataRead":27214,"rowCount":97,"jobId":9,"name":"toString at String.java:2994","description":"Delta: Job group for statement 13:\nif DeltaTable.isDeltaTable(spark,deltaTablePath):\n    deltaTable = DeltaTable.forPath(spark,deltaTablePath)\n    deltaTable.alias(\"t\").merge(\n        df2.alias(\"s\"),\n        f\"t.{tableKey} = s.{tableKey}\"\n    ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n    history = deltaTable.history(1).select(\"operationMetrics\")\n    operationMetrics = history.collect()[0][\"operationMetrics\"]\n    numInserted = operationMetrics[\"numTargetRowsInserted\"]\n    numUpdated = operationMetrics[\"numTargetRowsUpdated\"]\nelse:\n    df2.write.format(\"delta\").save(deltaTablePath)  \n    deltaTable = DeltaTable.forPath(spark,deltaTablePath)\n    operationMetrics = history.collect()[0][\"operationMetrics\"]\n    numInserted = operationMetrics[\"numTargetRowsInserted\"]\n    numUpdated = 0: Compute snapshot for version: 22","submissionTime":"2023-06-06T17:40:43.011GMT","completionTime":"2023-06-06T17:40:43.914GMT","stageIds":[12,13],"jobGroup":"13","status":"SUCCEEDED","numTasks":53,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":3,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":27214,"dataRead":55056,"rowCount":94,"jobId":8,"name":"toString at String.java:2994","description":"Delta: Job group for statement 13:\nif DeltaTable.isDeltaTable(spark,deltaTablePath):\n    deltaTable = DeltaTable.forPath(spark,deltaTablePath)\n    deltaTable.alias(\"t\").merge(\n        df2.alias(\"s\"),\n        f\"t.{tableKey} = s.{tableKey}\"\n    ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n    history = deltaTable.history(1).select(\"operationMetrics\")\n    operationMetrics = history.collect()[0][\"operationMetrics\"]\n    numInserted = operationMetrics[\"numTargetRowsInserted\"]\n    numUpdated = operationMetrics[\"numTargetRowsUpdated\"]\nelse:\n    df2.write.format(\"delta\").save(deltaTablePath)  \n    deltaTable = DeltaTable.forPath(spark,deltaTablePath)\n    operationMetrics = history.collect()[0][\"operationMetrics\"]\n    numInserted = operationMetrics[\"numTargetRowsInserted\"]\n    numUpdated = 0: Compute snapshot for version: 22","submissionTime":"2023-06-06T17:40:42.525GMT","completionTime":"2023-06-06T17:40:42.830GMT","stageIds":[11],"jobGroup":"13","status":"SUCCEEDED","numTasks":3,"numActiveTasks":0,"numCompletedTasks":3,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":3,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"ea1b0c1b-8a2c-4214-91e6-82f39f352aa1"},"text/plain":"StatementMeta(, e3bfaad7-54ed-4f11-a2bb-307e2a9d49f3, 13, Finished, Available)"},"metadata":{}}],"execution_count":11,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"2d446ebf-7fe9-4cd3-a0b1-6e9a081f702a"},{"cell_type":"markdown","source":["Get the latest date loaded into the table - this will be used for watermarking; return the max date, the number of rows inserted and number updated"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"1fe75275-c906-4c40-9968-6c65fce869c0"},{"cell_type":"code","source":["deltaTablePath = f\"{lakehousePath}/Tables/{tableName}\"\r\n","df3 = spark.read.format(\"delta\").load(deltaTablePath)\r\n","maxdate = df3.agg(max(dateColumn)).collect()[0][0]\r\n","# print(maxdate)\r\n","maxdate_str = maxdate.strftime(\"%Y-%m-%d %H:%M:%S\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"163b879d-ad53-4620-a4d6-068e682532b0","statement_id":8,"state":"finished","livy_statement_state":"available","queued_time":"2023-06-08T14:45:11.5962719Z","session_start_time":null,"execution_start_time":"2023-06-08T14:45:11.9050581Z","execution_finish_time":"2023-06-08T14:45:13.6347752Z","spark_jobs":{"numbers":{"FAILED":0,"SUCCEEDED":3,"UNKNOWN":0,"RUNNING":0},"jobs":[{"dataWritten":0,"dataRead":580,"rowCount":10,"jobId":18,"name":"collect at /tmp/ipykernel_9739/2064683289.py:3","description":"Job group for statement 8:\ndeltaTablePath = f\"{lakehousePath}/Tables/{tableName}\"\ndf3 = spark.read.format(\"delta\").load(deltaTablePath)\nmaxdate = df3.agg(max(dateColumn)).collect()[0][0]\n# print(maxdate)\nmaxdate_str = maxdate.strftime(\"%Y-%m-%d %H:%M:%S\")","submissionTime":"2023-06-08T14:45:13.210GMT","completionTime":"2023-06-08T14:45:13.225GMT","stageIds":[30,29],"jobGroup":"8","status":"SUCCEEDED","numTasks":11,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":10,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":580,"dataRead":5748729,"rowCount":7599626,"jobId":17,"name":"collect at /tmp/ipykernel_9739/2064683289.py:3","description":"Job group for statement 8:\ndeltaTablePath = f\"{lakehousePath}/Tables/{tableName}\"\ndf3 = spark.read.format(\"delta\").load(deltaTablePath)\nmaxdate = df3.agg(max(dateColumn)).collect()[0][0]\n# print(maxdate)\nmaxdate_str = maxdate.strftime(\"%Y-%m-%d %H:%M:%S\")","submissionTime":"2023-06-08T14:45:12.370GMT","completionTime":"2023-06-08T14:45:13.196GMT","stageIds":[28],"jobGroup":"8","status":"SUCCEEDED","numTasks":10,"numActiveTasks":0,"numCompletedTasks":10,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":10,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":36341,"rowCount":80,"jobId":16,"name":"collect at /tmp/ipykernel_9739/2064683289.py:3","description":"Delta: Job group for statement 8:\ndeltaTablePath = f\"{lakehousePath}/Tables/{tableName}\"\ndf3 = spark.read.format(\"delta\").load(deltaTablePath)\nmaxdate = df3.agg(max(dateColumn)).collect()[0][0]\n# print(maxdate)\nmaxdate_str = maxdate.strftime(\"%Y-%m-%d %H:%M:%S\"): Filtering files for query","submissionTime":"2023-06-08T14:45:12.101GMT","completionTime":"2023-06-08T14:45:12.305GMT","stageIds":[27,26],"jobGroup":"8","status":"SUCCEEDED","numTasks":58,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":8,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"4b6c4506-eac6-4d04-8de3-10d30dee583e"},"text/plain":"StatementMeta(, 163b879d-ad53-4620-a4d6-068e682532b0, 8, Finished, Available)"},"metadata":{}}],"execution_count":6,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"0dae64ac-4f61-4933-b087-96bab2ce6c31"},{"cell_type":"code","source":["result = \"maxdate=\"+maxdate_str +  \"|numInserted=\"+str(numInserted)+  \"|numUpdated=\"+str(numUpdated)\r\n","# result = {\"maxdate\": maxdate_str, \"numInserted\": numInserted, \"numUpdated\": numUpdated}\r\n","mssparkutils.notebook.exit(str(result))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"c0e5d901-5191-43b6-8b30-11c3928b5c85"},{"cell_type":"markdown","source":[],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"02260118-fdc5-4847-8886-784663c8f073"}]}